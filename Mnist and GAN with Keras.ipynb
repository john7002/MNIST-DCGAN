{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new data for Mnist datasets is the objective of this notebook. This is also a tutorial on GAN which is more and more use for generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GAN description ##\n",
    "\n",
    "I will not too much describe it here as there are many really good websites for, all based on 2014 paper.\n",
    "\n",
    "Basically two neural networks are competiting: a generator creates a picture, and the discriminator is trying to detect it as fake or not. We are injecting in the discrimator real picture et generated picture. After a while, generator starts generating pictures close to original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"./gan.png\",width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this my first contact with GAN, Ì will not try to produce exotic pictures, but will only use the boring Mnist dataset and apply general principles of GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Do it! ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the general steps I understood : \n",
    "    1. Generate few pictures with generator. This is gonna be pure random at first\n",
    "    2. Get the generated pictures and add same numbers from training set. From the created batch of picture, train the generator to discern fake from real picture.\n",
    "    3. After few steps of training, Discrimator is fixed and we create new picture in the generator, and we submit them to the discriminator. We optimize then the generator to make its pictures as real as possible.\n",
    "    4. Repeat last 3 steps, until created pictures are good quality (i.e state of equilibrium where Generator produces picture that discriminator can not discern from real picture, while discriminator one time over two find a fake picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the model, few interesting words from Siraj Ravel github and DCGAN paper:\n",
    "\n",
    "* Batch normalization is a must in both networks.\n",
    "* Fully hidden connected layers are not a good idea.\n",
    "* Avoid pooling, simply stride your convolutions!\n",
    "* Use ReLU activation in generator for all layers except for the output, which uses tanh. \n",
    "* Use LeakyReLU activation in the discriminator for all layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimax theorem that started the game theory states that for two players in a zero-sum game the minimax solution is the same as the Nash equilibrium.\n",
    "\n",
    "In simpler terms, when two players (D and G) are competing against each other (zero-sum game), and both play optimally assuming that their opponent is optimally (minimax strategy), the outcome is predetermined and none of the players can change it (Nash equilibrium).\n",
    "So, for our networks, it means that if we train them long enough, the Generator will learn how to sample from true “distribution,” meaning that it will start generating real life-like images, and the Discriminator will not be able to tell fake ones from genuine ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generator ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator is basically a \"de\"-convnet network that is outputting a picture with same dimensioins from the dataset. So let's first import the mnist data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Sequential, Model\n",
    "from keras.backend import resize_images\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "\n",
    "\n",
    "#mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# no need the labels, so we will gather all examples x.\n",
    "# Return a numpy array of shape (M, 28, 28)\n",
    "num_train, num_test = x_train.shape[0], x_test.shape[0]\n",
    "M = num_train + num_test\n",
    "x_all = np.zeros((M, 28, 28))\n",
    "x_all[:num_train, :, :] = x_train\n",
    "x_all[num_train:, :, :] = x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the data, what architecture for the Generator? For about a year after the first paper, training GANs seems difficult. In 2016 Radford et al. published the paper titled “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” describing the model that subsequently became famous as DCGAN. In the paper, the DCGAN architecture looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./gen_architecture.png\" width=\"700\" height=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./gen_architecture.png\",width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is composed of monochrome 28x28 pixel images, so the desired shape for our input layer is [batch_size, 28, 28, 1]. This will flatten the picture in a 1D array.\n",
    "\n",
    "Ok, so my understanding: The generated vector z has real values from -1 to 1 and follows the Gaussian distribution then :\n",
    "1. Input layer. Regarding z (100,1) : We should reshape it, but seem sybilin to me. (batch_size, 100,1,1) ? \n",
    "2. First step: Project and reshape: convert our input layer to 4x4x1024.\n",
    "3. 1st \"deconvnet layer\":the inverse of convolution, called transposed convolution, 512 filters of shape 5,5, stride=2  then relu activation. To calculate output shape of deconv layer: With padding=Same: H = H1 x stride if padding=Valid\n",
    "H = (H1-1) x stride + HF (H = output size, H1 = input size, HF = height of filter)\n",
    "4. 2nd \"deconvnet layer: 256 filters 5x5\n",
    "5. 3rd deconvnet layer: 128 filters 5,5\n",
    "6. 4th deconvnet layer: 1 filters 5,5 # woulb 3 filters for RGB picture\n",
    "\n",
    "As the output we want a 28x28 pictures. Meaning we have to change stride compares to original papers. If we keep using stride of 2, we will end up with 64x64 pictures. What we could try at first is just using a stride of 1 on first layer, in order to output 32x32 picture which is close to 28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    filter_size=[1024,512,256,128,1]\n",
    "    weight_init = RandomNormal(mean=0.0, stddev=0.02) # as suggested in paper\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(input_dim=100, output_dim=filter_size[0]*4*4,kernel_initializer=weight_init)) # out =dedim x dim x\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Reshape((4, 4, 1024)))  # here shape is (batch_size, 4, 4, 1024)\n",
    "\n",
    "    model.add(Conv2DTranspose(filters=filter_size[1], kernel_size=(5, 5), strides=(1,1), padding='same',kernel_initializer=weight_init))\n",
    "    # changed from 2,2 to make sure that picture dimension as output will be close to 28x28\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(momentum=0.9)) #conv1 output is shape (batch_size, 4, 4, 512)\n",
    "\n",
    "\n",
    "    model.add(Conv2DTranspose(filters=filter_size[2], kernel_size=(5, 5), strides=(2,2), padding='same',kernel_initializer=weight_init))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(momentum=0.9)) #conv2 output is shape (batch_size, 8, 8, 256)\n",
    "\n",
    "    model.add(Conv2DTranspose(filters=filter_size[3], kernel_size=(5, 5), strides=(2,2), padding='same',kernel_initializer=weight_init))\n",
    "    # changed from 2,2 to make sure that picture dimension as output will be close to 28x28\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(momentum=0.9)) #conv1 output is shape (batch_size, 16, 16, 128)\n",
    "\n",
    "    model.add(Conv2DTranspose(filters=filter_size[4], kernel_size=(5, 5), strides=(2,2), padding='same',kernel_initializer=weight_init))\n",
    "    # changed from 2,2 to make sure that picture dimension as output will be close to 28x28\n",
    "    model.add(Activation('tanh'))\n",
    "   # model.add(BatchNormalization(momentum=0.9)) #conv4 output is shape (batch_size, 32, 32, 1)\n",
    "\n",
    "    return model\n",
    "#RandomNormal(mean=0.0, stddev=0.05, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok now let's try to display a picture given a random Z as input, for a batch_size of 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, kernel_initializer=<keras.ini..., units=16384)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 4s 1s/step\n",
      "(3, 32, 32, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c0725e48>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "Z = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "gen = generator()\n",
    "gen.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "generated_images = gen.predict(Z, verbose=1)\n",
    "\n",
    "print(generated_images.shape)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(generated_images[0].reshape(32, 32), cmap=plt.cm.Greys)\n",
    "plt.title('Random picture 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(generated_images[1].reshape(32, 32), cmap=plt.cm.Greys)\n",
    "plt.title('Random picture 2')\n",
    "    \n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(generated_images[2].reshape(32, 32), cmap=plt.cm.Greys)\n",
    "plt.title('Random picture 3')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now we have a random picture generator, taking a [batch_size,100] array as input. There is a nice article about upsampling a picture, nice to read: https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Discriminator ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, this is the opposite of our generator. From a 32x32 picture, we will use the convnet to output a number between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./discriminator.png\" width=\"700\" height=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./discriminator.png\",width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the ouput size of a convolutional layer is $\\frac{n+2p-f}{s}+1 $ (n= input shape, p = padding, s = stride, f= filter size). \n",
    "Padding shape is:\n",
    "* \"Valid\" : n-f+1\n",
    "* \"Same\" : $p=\\frac{f-1}{2}$ ; n + 2p -f +1\n",
    "\n",
    "There is no common rules regarding, the filter size, just a 2^n size. If I keep same kernel size as the generator, I will end up with 2x2x1024 as output of the last conv layer. This is maybe not good, but let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    filter_size=[128,256,512,1024]\n",
    "    weight_init = RandomNormal(mean=0.0, stddev=0.02) # as suggested in paper\n",
    "    \n",
    "    in_shape = (32, 32,1)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=filter_size[0], kernel_size=(5, 5), strides=(2,2), padding='same',input_shape=in_shape,kernel_initializer=weight_init))\n",
    "    # padding will be : p = (f-1)/2 = 2 conv_1 output size is 32x 32 :32 + 2*2 - 5 +1 = 32\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    #model.add(BatchNormalization(momentum=0.9))  #paper does not recommand to add batchnorm to input layer\n",
    "    # out shape = > 32+ 2*2 - 5/2 = 15.5 (take the round ) => 15 + 1 => ouput size 16x16x 128\n",
    "    \n",
    "    \n",
    "    model.add(Conv2D(filters=filter_size[1], kernel_size=(5, 5), strides=(2,2), padding='same',kernel_initializer=weight_init))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization(momentum=0.9)) \n",
    "    # padding: p =2, out shape => 16 +4 -5 /2 = 7 => 7+1 => output size is 8x8x256\n",
    "    \n",
    "    model.add(Conv2D(filters=filter_size[2], kernel_size=(5, 5), strides=(2,2), padding='same',kernel_initializer=weight_init))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization(momentum=0.9)) \n",
    "    # padding: p =2, out shape => 8 +4 -5 /2 = 3.5 => 3+1 => output size is 4x4x512\n",
    "    \n",
    "    model.add(Conv2D(filters=filter_size[3], kernel_size=(5, 5), strides=(2,2), padding='same',kernel_initializer=weight_init))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(BatchNormalization(momentum=0.9)) \n",
    "    # padding: p =2, out shape => 4 +4 -5 /2 = 1.5 => 1+1 => output size is 2x2x1024\n",
    "\n",
    "    #Before we connect the layer, however, we'll flatten our feature map (conv_4) to shape [batch_size, features],\n",
    "    #so that our tensor has only two dimensions:\n",
    "    model.add(Flatten()) # output is 4096\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a random 32x32 picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "[[0.49755064]]\n"
     ]
    }
   ],
   "source": [
    "in1 = np.random.uniform(-1.0,1.0,[1, 32,32])\n",
    "random_picture = np.reshape(in1, (1, 32, 32, -1))\n",
    "\n",
    "discri = discriminator()\n",
    "discri.compile(loss='binary_crossentropy',optimizer='adam') \n",
    "return_label = discri.predict(random_picture, verbose=1)\n",
    "print(return_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay seems, we correctly generate a binary prediction based on a 32x32 input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we enter the real GAN mechanism. As a reminder we currently have a generator and a disriminator. How to handle the complete GAN process? Here are the main steps:\n",
    "1. Generate a batch of fake images thanks to our Generator\n",
    "2. Train the discriminator mixing the batch of fake image + a same number of real image. We also add the label 0=fake, 1=real. Optimize the weight of Discriminator network.\n",
    "3. Generate picture with the generator, infere them in the discriminator network and improve the genrator network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Generate batch of images ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a example, here is how we generate 64 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, units=16384)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 27ms/step\n",
      "(64, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "Z = np.random.uniform(-1, 1, (batch_size, 100))\n",
    "gen = generator()\n",
    "gen.compile(loss='binary_crossentropy', optimizer=\"adam\")\n",
    "generated_images = gen.predict(Z, verbose=1)\n",
    "\n",
    "print(generated_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Train the discriminator ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to train the discriminator:\n",
    "* when receiving real images, the discriminator should learn to compute high values (near 1), meaning that it is confident the input images are real\n",
    "* when receiving fake images, it should compute low values (near 0), meaning it is confident the input images are not real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Mnist dataset to feed the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tensor(numpy_data):\n",
    "    '''\n",
    "    numpy_data: (M, 28, 28), values in range [0, 255]\n",
    "    returns: tensor of images shaped [M, 32, 32, 1], with values in range [-1, 1]\n",
    "    '''\n",
    "    # resize images to 32x32\n",
    "    npad = ((0, 0), (2, 2), (2, 2))\n",
    "    X= np.pad(numpy_data,npad, 'constant', constant_values=(0, 0))\n",
    "    \n",
    "    X = np.expand_dims(X, axis=3) # add a dimension for number of chanel, here gray images, so only one chanel of 255 possibilities\n",
    "\n",
    "    # The data is currently in a range [0, 255].\n",
    "    # Transform data to have a range [-1, 1].\n",
    "    # We do this to match the range of tanh, the activation on the generator's output layer.\n",
    "    X = X / 128.\n",
    "    X = X - 1.\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Combined model ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the generator, we need to know the discriminator loss, so let's build a combined model which is just the genrator and then the denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_model(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we’re setting the discriminator’s training attribute to False before building the model. This means that for this model we will not be updating the weights of the discriminator during backpropagation. We will freeze these weights and only move the generator weights with the stack. The discriminator will be trained separately. Let's compile the models with the correct optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=100, kernel_initializer=<keras.ini..., units=16384)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "gen_optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "disc_optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "#disc_optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-10)\n",
    "gen = generator()\n",
    "discri = discriminator()\n",
    "\n",
    "discri.compile(loss='binary_crossentropy',optimizer=disc_optimizer,metrics=['accuracy'])\n",
    "gen.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "\n",
    "\n",
    "\n",
    "combined = stacked_model(gen,discri)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=gen_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that beta_1 = 0.5. This is a recommendation from the original DCGAN paper that we’ve carried forward and also had success with. A learning rate of 0.0002 is a good place to start, also recommended by DCGAN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_45 (Sequential)   (None, 32, 32, 1)         18931205  \n",
      "_________________________________________________________________\n",
      "sequential_46 (Sequential)   (None, 1)                 17220097  \n",
      "=================================================================\n",
      "Total params: 36,151,302\n",
      "Trainable params: 18,896,643\n",
      "Non-trainable params: 17,254,659\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the trainable params is not as big as the total params, due to the action to freeze the discriminator weights in combined model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Train the model ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to simply understand. Let's train it first on only one batch (X) of 64 real picture and 64 fake pictures and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# creating 64 fake images and 64 real images\n",
    "noise = np.random.normal(0, 1, (64, 100))\n",
    "fake_images = gen.predict(noise)\n",
    "fake_labels = np.zeros((64, 1))\n",
    "\n",
    "image_batch = x_all[0:64]\n",
    "real_images=data_tensor(image_batch)\n",
    "\n",
    "real_labels = np.ones((64, 1))\n",
    "\n",
    "print(real_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "# Train the discriminator (real classified as ones and generated as zeros)\n",
    "d_loss_real = discri.train_on_batch(real_images, real_labels)\n",
    "d_loss_fake = discri.train_on_batch(fake_images, fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator model has updated weight, thanks to real images and fake images. The model is theoritically better at discerning fake and real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4712086 0.25     ]\n",
      "0.14933929\n"
     ]
    }
   ],
   "source": [
    "d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 3.471209, acc.: 25.00%] [G loss: 0.000945]\n"
     ]
    }
   ],
   "source": [
    "noise = np.random.normal(0, 1, (64, 100))\n",
    "\n",
    "# Train the generator with fake images, pretending this is real ones.\n",
    "g_loss = combined.train_on_batch(noise, np.ones((64, 1)))\n",
    "\n",
    "# Plot the progress\n",
    "print(\"[D loss: %f, acc.: %.2f%%] [G loss: %f]\" %( d_loss[0], 100 * d_loss[1], g_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 A word about theory ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator’s loss function is the cross entropy loss function. To understand this, let’s suppose we’re doing some binary classification with some trainable function D that we wish to optimize, where D\n",
    "D indicates the estimated probability of some data point $x_{i}$ being in the first class. To get the predicted probability of being in the second class, we just do 1-D($x_{i}$).\n",
    "\n",
    "The cross entropy between two distributions, which we’ll call p and q, is defined as:\n",
    "\n",
    "$H(p,q) = - \\sum(p_{i} log(q_{i})$\n",
    "\n",
    "To apply this loss function to the current binary classification task, we define the true distribution as $P[y_{i}=0]=1$ if $y_{i}=0$ and $P[y_{i}=1]=1$ if $y_{i}=1$\n",
    "\n",
    "Thus, for one data point x1 and its label, we get the following loss function:\n",
    "\n",
    "$H(x1,y1) = -y1.log(D(x1)-(1-y1)log(1-D(x1))$\n",
    "\n",
    "Let’s look at the above function. Notice that only one of the two terms is going to be zero, depending on the value of y1 , which makes sense since it’s defining a distribution which is either [0,1] or [1,0].\n",
    "\n",
    "\n",
    "Have a look here https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6 Complete training ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace everything in complete training. We will use batch of 64 images. Also introduce a function to save the generated pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(generator, epoch, batch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d_%d.png\" % (epoch, batch))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0/546 [D loss: 3.701529, acc.: 35.16%] [G loss: 1.796620]\n",
      "Epoch 0 Batch 1/546 [D loss: 0.309367, acc.: 85.16%] [G loss: 5.773452]\n",
      "Epoch 0 Batch 2/546 [D loss: 0.027594, acc.: 100.00%] [G loss: 6.901357]\n",
      "Epoch 0 Batch 3/546 [D loss: 0.008008, acc.: 100.00%] [G loss: 6.646492]\n",
      "Epoch 0 Batch 4/546 [D loss: 0.063209, acc.: 97.66%] [G loss: 7.772598]\n",
      "Epoch 0 Batch 5/546 [D loss: 0.000700, acc.: 100.00%] [G loss: 10.238914]\n",
      "Epoch 0 Batch 6/546 [D loss: 0.003085, acc.: 100.00%] [G loss: 9.356950]\n",
      "Epoch 0 Batch 7/546 [D loss: 0.009628, acc.: 99.22%] [G loss: 8.286095]\n",
      "Epoch 0 Batch 8/546 [D loss: 0.002775, acc.: 100.00%] [G loss: 7.172940]\n",
      "Epoch 0 Batch 9/546 [D loss: 0.003542, acc.: 100.00%] [G loss: 6.990102]\n",
      "Epoch 0 Batch 10/546 [D loss: 0.010367, acc.: 100.00%] [G loss: 8.128311]\n",
      "Epoch 0 Batch 11/546 [D loss: 0.003518, acc.: 100.00%] [G loss: 8.311946]\n",
      "Epoch 0 Batch 12/546 [D loss: 0.002743, acc.: 100.00%] [G loss: 7.950078]\n",
      "Epoch 0 Batch 13/546 [D loss: 0.005713, acc.: 100.00%] [G loss: 8.352112]\n",
      "Epoch 0 Batch 14/546 [D loss: 0.008359, acc.: 100.00%] [G loss: 8.034541]\n",
      "Epoch 0 Batch 15/546 [D loss: 0.001148, acc.: 100.00%] [G loss: 7.861476]\n",
      "Epoch 0 Batch 16/546 [D loss: 0.002776, acc.: 100.00%] [G loss: 7.945015]\n",
      "Epoch 0 Batch 17/546 [D loss: 0.004072, acc.: 100.00%] [G loss: 8.714655]\n",
      "Epoch 0 Batch 18/546 [D loss: 0.001056, acc.: 100.00%] [G loss: 8.880754]\n",
      "Epoch 0 Batch 19/546 [D loss: 0.002206, acc.: 100.00%] [G loss: 8.017985]\n",
      "Epoch 0 Batch 20/546 [D loss: 0.001840, acc.: 100.00%] [G loss: 7.792753]\n",
      "Epoch 0 Batch 21/546 [D loss: 0.003406, acc.: 100.00%] [G loss: 9.243647]\n",
      "Epoch 0 Batch 22/546 [D loss: 0.000381, acc.: 100.00%] [G loss: 9.355813]\n",
      "Epoch 0 Batch 23/546 [D loss: 0.000484, acc.: 100.00%] [G loss: 8.990328]\n",
      "Epoch 0 Batch 24/546 [D loss: 0.001345, acc.: 100.00%] [G loss: 8.582198]\n",
      "Epoch 0 Batch 25/546 [D loss: 0.001795, acc.: 100.00%] [G loss: 8.814812]\n",
      "Epoch 0 Batch 26/546 [D loss: 0.001001, acc.: 100.00%] [G loss: 9.160999]\n",
      "Epoch 0 Batch 27/546 [D loss: 0.000566, acc.: 100.00%] [G loss: 8.677185]\n",
      "Epoch 0 Batch 28/546 [D loss: 0.001272, acc.: 100.00%] [G loss: 8.327439]\n",
      "Epoch 0 Batch 29/546 [D loss: 0.001339, acc.: 100.00%] [G loss: 8.144601]\n",
      "Epoch 0 Batch 30/546 [D loss: 0.001260, acc.: 100.00%] [G loss: 8.398496]\n",
      "Epoch 0 Batch 31/546 [D loss: 0.000758, acc.: 100.00%] [G loss: 7.890778]\n",
      "Epoch 0 Batch 32/546 [D loss: 0.002056, acc.: 100.00%] [G loss: 8.812812]\n",
      "Epoch 0 Batch 33/546 [D loss: 0.000499, acc.: 100.00%] [G loss: 9.293765]\n",
      "Epoch 0 Batch 34/546 [D loss: 0.000424, acc.: 100.00%] [G loss: 9.280395]\n",
      "Epoch 0 Batch 35/546 [D loss: 0.001064, acc.: 100.00%] [G loss: 8.512186]\n",
      "Epoch 0 Batch 36/546 [D loss: 0.001855, acc.: 100.00%] [G loss: 9.096832]\n",
      "Epoch 0 Batch 37/546 [D loss: 0.000692, acc.: 100.00%] [G loss: 9.220984]\n",
      "Epoch 0 Batch 38/546 [D loss: 0.001089, acc.: 100.00%] [G loss: 9.021055]\n",
      "Epoch 0 Batch 39/546 [D loss: 0.000640, acc.: 100.00%] [G loss: 8.981522]\n",
      "Epoch 0 Batch 40/546 [D loss: 0.001222, acc.: 100.00%] [G loss: 9.053504]\n",
      "Epoch 0 Batch 41/546 [D loss: 0.000713, acc.: 100.00%] [G loss: 9.165728]\n",
      "Epoch 0 Batch 42/546 [D loss: 0.001297, acc.: 100.00%] [G loss: 8.728927]\n",
      "Epoch 0 Batch 43/546 [D loss: 0.000814, acc.: 100.00%] [G loss: 8.739355]\n",
      "Epoch 0 Batch 44/546 [D loss: 0.002153, acc.: 100.00%] [G loss: 8.512358]\n",
      "Epoch 0 Batch 45/546 [D loss: 0.001734, acc.: 100.00%] [G loss: 10.011356]\n",
      "Epoch 0 Batch 46/546 [D loss: 0.000309, acc.: 100.00%] [G loss: 10.595941]\n",
      "Epoch 0 Batch 47/546 [D loss: 0.001014, acc.: 100.00%] [G loss: 9.789447]\n",
      "Epoch 0 Batch 48/546 [D loss: 0.000863, acc.: 100.00%] [G loss: 9.429514]\n",
      "Epoch 0 Batch 49/546 [D loss: 0.001458, acc.: 100.00%] [G loss: 9.961214]\n",
      "Epoch 0 Batch 50/546 [D loss: 0.000381, acc.: 100.00%] [G loss: 10.315454]\n",
      "Epoch 0 Batch 51/546 [D loss: 0.000874, acc.: 100.00%] [G loss: 9.956664]\n",
      "Epoch 0 Batch 52/546 [D loss: 0.004688, acc.: 100.00%] [G loss: 9.076950]\n",
      "Epoch 0 Batch 53/546 [D loss: 0.001718, acc.: 100.00%] [G loss: 10.165127]\n",
      "Epoch 0 Batch 54/546 [D loss: 0.000139, acc.: 100.00%] [G loss: 10.599961]\n",
      "Epoch 0 Batch 55/546 [D loss: 0.000147, acc.: 100.00%] [G loss: 10.021760]\n",
      "Epoch 0 Batch 56/546 [D loss: 0.001477, acc.: 100.00%] [G loss: 9.920759]\n",
      "Epoch 0 Batch 57/546 [D loss: 0.000924, acc.: 100.00%] [G loss: 10.501108]\n",
      "Epoch 0 Batch 58/546 [D loss: 0.000363, acc.: 100.00%] [G loss: 10.203779]\n",
      "Epoch 0 Batch 59/546 [D loss: 0.000582, acc.: 100.00%] [G loss: 10.225899]\n",
      "Epoch 0 Batch 60/546 [D loss: 0.000473, acc.: 100.00%] [G loss: 9.996183]\n",
      "Epoch 0 Batch 61/546 [D loss: 0.000377, acc.: 100.00%] [G loss: 9.403718]\n",
      "Epoch 0 Batch 62/546 [D loss: 0.001099, acc.: 100.00%] [G loss: 10.077898]\n",
      "Epoch 0 Batch 63/546 [D loss: 0.000550, acc.: 100.00%] [G loss: 9.702263]\n",
      "Epoch 0 Batch 64/546 [D loss: 0.000569, acc.: 100.00%] [G loss: 9.233267]\n",
      "Epoch 0 Batch 65/546 [D loss: 0.000451, acc.: 100.00%] [G loss: 9.090609]\n",
      "Epoch 0 Batch 66/546 [D loss: 0.001072, acc.: 100.00%] [G loss: 9.540394]\n",
      "Epoch 0 Batch 67/546 [D loss: 0.000614, acc.: 100.00%] [G loss: 9.737473]\n",
      "Epoch 0 Batch 68/546 [D loss: 0.000537, acc.: 100.00%] [G loss: 9.924791]\n",
      "Epoch 0 Batch 69/546 [D loss: 0.000479, acc.: 100.00%] [G loss: 9.459369]\n",
      "Epoch 0 Batch 70/546 [D loss: 0.002370, acc.: 100.00%] [G loss: 9.444404]\n",
      "Epoch 0 Batch 71/546 [D loss: 0.000612, acc.: 100.00%] [G loss: 9.969378]\n",
      "Epoch 0 Batch 72/546 [D loss: 0.000314, acc.: 100.00%] [G loss: 9.953496]\n",
      "Epoch 0 Batch 73/546 [D loss: 0.001214, acc.: 100.00%] [G loss: 8.998852]\n",
      "Epoch 0 Batch 74/546 [D loss: 0.003592, acc.: 100.00%] [G loss: 12.418259]\n",
      "Epoch 0 Batch 75/546 [D loss: 0.002091, acc.: 100.00%] [G loss: 13.043001]\n",
      "Epoch 0 Batch 76/546 [D loss: 0.000170, acc.: 100.00%] [G loss: 12.543843]\n",
      "Epoch 0 Batch 77/546 [D loss: 0.000138, acc.: 100.00%] [G loss: 11.317266]\n",
      "Epoch 0 Batch 78/546 [D loss: 0.000745, acc.: 100.00%] [G loss: 11.004106]\n",
      "Epoch 0 Batch 79/546 [D loss: 0.000093, acc.: 100.00%] [G loss: 11.471688]\n",
      "Epoch 0 Batch 80/546 [D loss: 0.000349, acc.: 100.00%] [G loss: 10.766404]\n",
      "Epoch 0 Batch 81/546 [D loss: 0.000241, acc.: 100.00%] [G loss: 9.939514]\n",
      "Epoch 0 Batch 82/546 [D loss: 0.001428, acc.: 100.00%] [G loss: 11.674746]\n",
      "Epoch 0 Batch 83/546 [D loss: 0.000297, acc.: 100.00%] [G loss: 11.649562]\n",
      "Epoch 0 Batch 84/546 [D loss: 0.000528, acc.: 100.00%] [G loss: 10.332234]\n",
      "Epoch 0 Batch 85/546 [D loss: 0.000444, acc.: 100.00%] [G loss: 9.248220]\n",
      "Epoch 0 Batch 86/546 [D loss: 0.000986, acc.: 100.00%] [G loss: 9.924685]\n",
      "Epoch 0 Batch 87/546 [D loss: 0.000120, acc.: 100.00%] [G loss: 10.385340]\n",
      "Epoch 0 Batch 88/546 [D loss: 0.000627, acc.: 100.00%] [G loss: 10.256158]\n",
      "Epoch 0 Batch 89/546 [D loss: 0.000475, acc.: 100.00%] [G loss: 9.843728]\n",
      "Epoch 0 Batch 90/546 [D loss: 0.000400, acc.: 100.00%] [G loss: 9.557654]\n",
      "Epoch 0 Batch 91/546 [D loss: 0.001513, acc.: 100.00%] [G loss: 10.583691]\n",
      "Epoch 0 Batch 92/546 [D loss: 0.001372, acc.: 100.00%] [G loss: 10.642496]\n",
      "Epoch 0 Batch 93/546 [D loss: 0.002374, acc.: 100.00%] [G loss: 12.071137]\n",
      "Epoch 0 Batch 94/546 [D loss: 0.001539, acc.: 100.00%] [G loss: 11.191402]\n",
      "Epoch 0 Batch 95/546 [D loss: 0.000139, acc.: 100.00%] [G loss: 10.394823]\n",
      "Epoch 0 Batch 96/546 [D loss: 0.000126, acc.: 100.00%] [G loss: 9.919340]\n",
      "Epoch 0 Batch 97/546 [D loss: 0.000353, acc.: 100.00%] [G loss: 9.315969]\n",
      "Epoch 0 Batch 98/546 [D loss: 0.000525, acc.: 100.00%] [G loss: 10.498346]\n",
      "Epoch 0 Batch 99/546 [D loss: 0.000379, acc.: 100.00%] [G loss: 10.692037]\n",
      "Epoch 0 Batch 100/546 [D loss: 0.000253, acc.: 100.00%] [G loss: 10.622123]\n",
      "Epoch 0 Batch 101/546 [D loss: 0.000415, acc.: 100.00%] [G loss: 10.609385]\n",
      "Epoch 0 Batch 102/546 [D loss: 0.000481, acc.: 100.00%] [G loss: 10.913082]\n",
      "Epoch 0 Batch 103/546 [D loss: 0.000374, acc.: 100.00%] [G loss: 10.416903]\n",
      "Epoch 0 Batch 104/546 [D loss: 0.000204, acc.: 100.00%] [G loss: 9.977234]\n",
      "Epoch 0 Batch 105/546 [D loss: 0.000561, acc.: 100.00%] [G loss: 10.045425]\n",
      "Epoch 0 Batch 106/546 [D loss: 0.000273, acc.: 100.00%] [G loss: 9.724414]\n",
      "Epoch 0 Batch 107/546 [D loss: 0.001857, acc.: 100.00%] [G loss: 9.814541]\n",
      "Epoch 0 Batch 108/546 [D loss: 0.000805, acc.: 100.00%] [G loss: 10.794372]\n",
      "Epoch 0 Batch 109/546 [D loss: 0.003365, acc.: 100.00%] [G loss: 13.426016]\n",
      "Epoch 0 Batch 110/546 [D loss: 0.001492, acc.: 100.00%] [G loss: 13.149441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 111/546 [D loss: 0.000123, acc.: 100.00%] [G loss: 11.985822]\n",
      "Epoch 0 Batch 112/546 [D loss: 0.000374, acc.: 100.00%] [G loss: 10.638206]\n",
      "Epoch 0 Batch 113/546 [D loss: 0.001454, acc.: 100.00%] [G loss: 13.435026]\n",
      "Epoch 0 Batch 114/546 [D loss: 0.000609, acc.: 100.00%] [G loss: 13.183672]\n",
      "Epoch 0 Batch 115/546 [D loss: 0.000711, acc.: 100.00%] [G loss: 10.667036]\n",
      "Epoch 0 Batch 116/546 [D loss: 0.000468, acc.: 100.00%] [G loss: 9.414034]\n",
      "Epoch 0 Batch 117/546 [D loss: 0.011453, acc.: 99.22%] [G loss: 15.671572]\n",
      "Epoch 0 Batch 118/546 [D loss: 9.992313, acc.: 1.56%] [G loss: 0.065303]\n",
      "Epoch 0 Batch 119/546 [D loss: 7.420432, acc.: 51.56%] [G loss: 1.611216]\n",
      "Epoch 0 Batch 120/546 [D loss: 6.320627, acc.: 58.59%] [G loss: 2.970214]\n",
      "Epoch 0 Batch 121/546 [D loss: 6.535312, acc.: 57.03%] [G loss: 4.810687]\n",
      "Epoch 0 Batch 122/546 [D loss: 3.622310, acc.: 73.44%] [G loss: 15.510130]\n",
      "Epoch 0 Batch 123/546 [D loss: 6.727485, acc.: 50.00%] [G loss: 16.109451]\n",
      "Epoch 0 Batch 124/546 [D loss: 4.481023, acc.: 65.62%] [G loss: 15.376215]\n",
      "Epoch 0 Batch 125/546 [D loss: 2.208237, acc.: 68.75%] [G loss: 16.118095]\n",
      "Epoch 0 Batch 126/546 [D loss: 3.222889, acc.: 67.97%] [G loss: 15.492182]\n",
      "Epoch 0 Batch 127/546 [D loss: 0.001931, acc.: 100.00%] [G loss: 11.517714]\n",
      "Epoch 0 Batch 128/546 [D loss: 1.907102, acc.: 74.22%] [G loss: 15.897739]\n",
      "Epoch 0 Batch 129/546 [D loss: 0.335427, acc.: 90.62%] [G loss: 15.575638]\n",
      "Epoch 0 Batch 130/546 [D loss: 0.026674, acc.: 98.44%] [G loss: 14.960218]\n",
      "Epoch 0 Batch 131/546 [D loss: 0.272639, acc.: 96.88%] [G loss: 15.101886]\n",
      "Epoch 0 Batch 132/546 [D loss: 0.387861, acc.: 93.75%] [G loss: 15.324511]\n",
      "Epoch 0 Batch 133/546 [D loss: 0.001842, acc.: 100.00%] [G loss: 15.289890]\n",
      "Epoch 0 Batch 134/546 [D loss: 0.270124, acc.: 96.88%] [G loss: 14.162569]\n",
      "Epoch 0 Batch 135/546 [D loss: 0.246039, acc.: 96.88%] [G loss: 14.678389]\n",
      "Epoch 0 Batch 136/546 [D loss: 0.013491, acc.: 99.22%] [G loss: 14.723139]\n",
      "Epoch 0 Batch 137/546 [D loss: 0.027215, acc.: 98.44%] [G loss: 13.961494]\n",
      "Epoch 0 Batch 138/546 [D loss: 0.025538, acc.: 99.22%] [G loss: 13.029551]\n",
      "Epoch 0 Batch 139/546 [D loss: 0.222963, acc.: 96.88%] [G loss: 14.333834]\n",
      "Epoch 0 Batch 140/546 [D loss: 0.036539, acc.: 99.22%] [G loss: 13.520002]\n",
      "Epoch 0 Batch 141/546 [D loss: 0.010291, acc.: 100.00%] [G loss: 13.913671]\n",
      "Epoch 0 Batch 142/546 [D loss: 0.004311, acc.: 100.00%] [G loss: 12.929697]\n",
      "Epoch 0 Batch 143/546 [D loss: 0.128294, acc.: 96.09%] [G loss: 15.250785]\n",
      "Epoch 0 Batch 144/546 [D loss: 0.000959, acc.: 100.00%] [G loss: 15.942458]\n",
      "Epoch 0 Batch 145/546 [D loss: 0.024701, acc.: 99.22%] [G loss: 15.867393]\n",
      "Epoch 0 Batch 146/546 [D loss: 0.000060, acc.: 100.00%] [G loss: 15.614662]\n",
      "Epoch 0 Batch 147/546 [D loss: 0.000019, acc.: 100.00%] [G loss: 15.474714]\n",
      "Epoch 0 Batch 148/546 [D loss: 0.031384, acc.: 99.22%] [G loss: 15.647181]\n",
      "Epoch 0 Batch 149/546 [D loss: 0.000046, acc.: 100.00%] [G loss: 15.492749]\n",
      "Epoch 0 Batch 150/546 [D loss: 0.000059, acc.: 100.00%] [G loss: 15.754398]\n",
      "Epoch 0 Batch 151/546 [D loss: 0.000576, acc.: 100.00%] [G loss: 15.487557]\n",
      "Epoch 0 Batch 152/546 [D loss: 0.000459, acc.: 100.00%] [G loss: 14.728682]\n",
      "Epoch 0 Batch 153/546 [D loss: 0.000893, acc.: 100.00%] [G loss: 14.401762]\n",
      "Epoch 0 Batch 154/546 [D loss: 0.097562, acc.: 98.44%] [G loss: 15.171959]\n",
      "Epoch 0 Batch 155/546 [D loss: 0.000032, acc.: 100.00%] [G loss: 15.734911]\n",
      "Epoch 0 Batch 156/546 [D loss: 0.001750, acc.: 100.00%] [G loss: 15.582689]\n",
      "Epoch 0 Batch 157/546 [D loss: 0.000063, acc.: 100.00%] [G loss: 15.146572]\n",
      "Epoch 0 Batch 158/546 [D loss: 0.000114, acc.: 100.00%] [G loss: 14.680584]\n",
      "Epoch 0 Batch 159/546 [D loss: 0.211450, acc.: 93.75%] [G loss: 14.934046]\n",
      "Epoch 0 Batch 160/546 [D loss: 0.015337, acc.: 99.22%] [G loss: 15.068687]\n",
      "Epoch 0 Batch 161/546 [D loss: 0.053820, acc.: 99.22%] [G loss: 14.738453]\n",
      "Epoch 0 Batch 162/546 [D loss: 0.034411, acc.: 99.22%] [G loss: 15.485743]\n",
      "Epoch 0 Batch 163/546 [D loss: 0.000651, acc.: 100.00%] [G loss: 15.755921]\n",
      "Epoch 0 Batch 164/546 [D loss: 0.001147, acc.: 100.00%] [G loss: 15.696898]\n",
      "Epoch 0 Batch 165/546 [D loss: 0.000084, acc.: 100.00%] [G loss: 15.226709]\n",
      "Epoch 0 Batch 166/546 [D loss: 0.002936, acc.: 100.00%] [G loss: 15.084415]\n",
      "Epoch 0 Batch 167/546 [D loss: 0.000190, acc.: 100.00%] [G loss: 14.355925]\n",
      "Epoch 0 Batch 168/546 [D loss: 0.003043, acc.: 100.00%] [G loss: 13.855747]\n",
      "Epoch 0 Batch 169/546 [D loss: 0.000443, acc.: 100.00%] [G loss: 12.923834]\n",
      "Epoch 0 Batch 170/546 [D loss: 0.011859, acc.: 99.22%] [G loss: 13.339739]\n",
      "Epoch 0 Batch 171/546 [D loss: 0.000616, acc.: 100.00%] [G loss: 13.567037]\n",
      "Epoch 0 Batch 172/546 [D loss: 0.002042, acc.: 100.00%] [G loss: 11.967731]\n",
      "Epoch 0 Batch 173/546 [D loss: 0.003752, acc.: 100.00%] [G loss: 10.934828]\n",
      "Epoch 0 Batch 174/546 [D loss: 0.023732, acc.: 98.44%] [G loss: 14.006554]\n",
      "Epoch 0 Batch 175/546 [D loss: 0.003054, acc.: 100.00%] [G loss: 14.963295]\n",
      "Epoch 0 Batch 176/546 [D loss: 0.016971, acc.: 100.00%] [G loss: 13.060362]\n",
      "Epoch 0 Batch 177/546 [D loss: 0.001947, acc.: 100.00%] [G loss: 12.241219]\n",
      "Epoch 0 Batch 178/546 [D loss: 0.010646, acc.: 100.00%] [G loss: 13.893412]\n",
      "Epoch 0 Batch 179/546 [D loss: 0.001178, acc.: 100.00%] [G loss: 14.456868]\n",
      "Epoch 0 Batch 180/546 [D loss: 0.002101, acc.: 100.00%] [G loss: 13.522175]\n",
      "Epoch 0 Batch 181/546 [D loss: 0.002250, acc.: 100.00%] [G loss: 11.441246]\n",
      "Epoch 0 Batch 182/546 [D loss: 0.030550, acc.: 99.22%] [G loss: 15.390481]\n",
      "Epoch 0 Batch 183/546 [D loss: 0.027982, acc.: 100.00%] [G loss: 10.700146]\n",
      "Epoch 0 Batch 184/546 [D loss: 0.058680, acc.: 97.66%] [G loss: 16.095751]\n",
      "Epoch 0 Batch 185/546 [D loss: 0.015594, acc.: 99.22%] [G loss: 16.118095]\n",
      "Epoch 0 Batch 186/546 [D loss: 0.023827, acc.: 100.00%] [G loss: 16.030657]\n",
      "Epoch 0 Batch 187/546 [D loss: 0.000048, acc.: 100.00%] [G loss: 15.559109]\n",
      "Epoch 0 Batch 188/546 [D loss: 0.000004, acc.: 100.00%] [G loss: 15.045643]\n",
      "Epoch 0 Batch 189/546 [D loss: 0.000042, acc.: 100.00%] [G loss: 13.828876]\n",
      "Epoch 0 Batch 190/546 [D loss: 0.000201, acc.: 100.00%] [G loss: 11.527761]\n",
      "Epoch 0 Batch 191/546 [D loss: 0.012401, acc.: 99.22%] [G loss: 12.892576]\n",
      "Epoch 0 Batch 192/546 [D loss: 0.000065, acc.: 100.00%] [G loss: 13.321100]\n",
      "Epoch 0 Batch 193/546 [D loss: 0.000193, acc.: 100.00%] [G loss: 11.780257]\n",
      "Epoch 0 Batch 194/546 [D loss: 0.000903, acc.: 100.00%] [G loss: 9.110692]\n",
      "Epoch 0 Batch 195/546 [D loss: 0.073314, acc.: 96.88%] [G loss: 16.118095]\n",
      "Epoch 0 Batch 196/546 [D loss: 13.041522, acc.: 0.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 197/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 198/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 199/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 200/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 201/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 202/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 203/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 204/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 205/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 206/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 207/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 208/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 209/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 210/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 211/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 212/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 213/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 214/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 215/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 216/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 217/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 218/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 219/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 220/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 221/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 222/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 223/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 224/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 225/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 226/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 227/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 228/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 229/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 230/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 231/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 232/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 233/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 234/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 235/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 236/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 237/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 238/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 239/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 240/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 241/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 242/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 243/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 244/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 245/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 246/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 247/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 248/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 249/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "Epoch 0 Batch 250/546 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-3ad13fd6d7e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m      \u001b[0;31m# train the discriminator on both real and fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m   1066\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "num_examples = x_all.shape[0] # around 70k examples.\n",
    "num_batches = int(num_examples / float(batch_size)) # around 1093 batches\n",
    "half_batch = int(batch_size / 2) # 32 pictures\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    for batch in range(num_batches):\n",
    "    # creating the real and fake picture for this batch\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        fake_images = gen.predict(noise)\n",
    "        fake_labels = np.zeros((half_batch, 1))\n",
    "\n",
    "        image_batch = x_all[batch*half_batch:batch*half_batch+half_batch]\n",
    "        real_images=data_tensor(image_batch)\n",
    "\n",
    "        real_labels = np.ones((half_batch, 1))\n",
    "        \n",
    "     # train the discriminator on both real and fake images\n",
    "        d_loss_real = discri.train_on_batch(real_images, real_labels)\n",
    "        d_loss_fake = discri.train_on_batch(fake_images, fake_labels)\n",
    "    \n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        \n",
    "    # train the generator\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        # Train the generator with fake images, pretending this is real ones.\n",
    "        g_loss = combined.train_on_batch(noise, np.ones((half_batch, 1)))\n",
    "            \n",
    "        \n",
    "        print(\"Epoch %d Batch %d/%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,batch, num_batches, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "        if batch % 50 == 0:\n",
    "            save_imgs(gen, epoch, batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several epochs, this is how the generated images are evolving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./animation.gif\" width=\"700\" height=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./animation.gif\",width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the last pictures generated after 14 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./mnist_14_0.png\" width=\"700\" height=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./mnist_14_0.png\",width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad. Surely can be improved with parameters tuning and mlaybe train longer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
