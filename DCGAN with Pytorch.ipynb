{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new data for Mnist datasets and then Celebrity face is the objective of this notebook. This is also a tutorial on GAN which is more and more use for generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GAN description ##\n",
    "\n",
    "I will not too much describe it here as there are many really good websites for, all based on 2014 paper.\n",
    "\n",
    "Basically two neural networks are competiting: a generator creates a picture, and the discriminator is trying to detect it as fake or not. We are injecting in the discrimator real picture et generated picture. After a while, generator starts generating pictures close to original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./gan.png\" width=\"700\" height=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"./gan.png\",width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this my first contact with GAN, Ì will not try to produce exotic pictures, but will only use the boring Mnist dataset at first time  and apply general principles of GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Do it! ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the general steps I understood : \n",
    "    1. Generate few pictures with generator. This is gonna be pure random at first\n",
    "    2. Get the generated pictures and add same numbers from training set. From the created batch of picture, train the generator to discern fake from real picture.\n",
    "    3. After few steps of training, Discrimator is fixed and we create new picture in the generator, and we submit them to the discriminator. We optimize then the generator to make its pictures as real as possible.\n",
    "    4. Repeat last 3 steps, until created pictures are good quality (i.e state of equilibrium where Generator produces picture that discriminator can not discern from real picture, while discriminator one time over two find a fake picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the model, few interesting words from different interesting github repos and DCGAN paper:\n",
    "\n",
    "* Batch normalization is a must in both networks.\n",
    "* Fully hidden connected layers are not a good idea.\n",
    "* Avoid pooling, simply stride your convolutions!\n",
    "* Use ReLU activation in generator for all layers except for the output, which uses tanh. \n",
    "* Use LeakyReLU activation in the discriminator for all layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimax theorem that started the game theory states that for two players in a zero-sum game the minimax solution is the same as the Nash equilibrium.\n",
    "\n",
    "In simpler terms, when two players (D and G) are competing against each other (zero-sum game), and both play optimally assuming that their opponent is optimally (minimax strategy), the outcome is predetermined and none of the players can change it (Nash equilibrium).\n",
    "So, for our networks, it means that if we train them long enough, the Generator will learn how to sample from true “distribution,” meaning that it will start generating real life-like images, and the Discriminator will not be able to tell fake ones from genuine ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generator ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator is basically a \"de\"-convnet network that is outputting a picture with same dimensions from the dataset. So let's first import the mnist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import uniform\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "trans = transforms.Compose([transforms.Pad(2),transforms.ToTensor(),transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True,transform=trans)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported 15000 batches of 4 pictures.(i.e total is 60k pictures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the data, what architecture for the Generator? For about a year after the first paper, training GANs seems difficult. In 2016 Radford et al. published the paper titled “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks” describing the model that subsequently became famous as DCGAN. In the paper, the DCGAN architecture looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./gen_architecture.png\" width=\"700\" height=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./gen_architecture.png\",width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset is composed of monochrome 28x28 pixel images, so the desired shape for our input layer is [batch_size, 28, 28, 1]. This will flatten the picture in a 1D array.\n",
    "\n",
    "Ok, so my understanding: The generated vector z has real values from -1 to 1 and follows the Gaussian distribution then :\n",
    "1. Input layer. Regarding z (100,1) : We should reshape it, but seem sybilin to me. (batch_size, 100,1,1) ? \n",
    "2. First step: Project and reshape: convert our input layer to 4x4x1024.\n",
    "3. 1st \"deconvnet layer\":the inverse of convolution, called transposed convolution, 512 filters of shape 5,5, stride=2  then relu activation. To calculate output shape of deconv layer: With padding=Same: H = H1 x stride if padding=Valid\n",
    "H = (H1-1) x stride + HF (H = output size, H1 = input size, HF = height of filter)\n",
    "4. 2nd \"deconvnet layer: 256 filters 5x5\n",
    "5. 3rd deconvnet layer: 128 filters 5,5\n",
    "6. 4th deconvnet layer: 1 filters 5,5 # woulb 3 filters for RGB picture\n",
    "\n",
    "As the output we want a 28x28 pictures. Meaning we have to change stride compares to original papers. If we keep using stride of 2, we will end up with 64x64 pictures. What we could try at first is just using a stride of 1 on first layer, in order to output 32x32 picture which is close to 28x28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the DCGAN paper, the authors specify that all model weights shall be randomly initialized from a Normal distribution with mean=0, stdev=0.2. The weights_init function takes an initialized model as input and reinitializes all convolutional, convolutional-transpose, and batch normalization layers to meet this criteria. This function is applied to the models immediately after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (Remember : Order in Pytorch shape is (N,Cin,H,W) and output (N,Cout,Hout,Wout) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1          [-1, 1, 1, 16384]       1,654,784\n",
      "       BatchNorm2d-2           [-1, 1024, 4, 4]           2,048\n",
      "              ReLU-3           [-1, 1024, 4, 4]               0\n",
      "   ConvTranspose2d-4            [-1, 512, 4, 4]      13,107,200\n",
      "       BatchNorm2d-5            [-1, 512, 4, 4]           1,024\n",
      "              ReLU-6            [-1, 512, 4, 4]               0\n",
      "   ConvTranspose2d-7            [-1, 256, 8, 8]       3,276,800\n",
      "       BatchNorm2d-8            [-1, 256, 8, 8]             512\n",
      "              ReLU-9            [-1, 256, 8, 8]               0\n",
      "  ConvTranspose2d-10          [-1, 128, 16, 16]         819,200\n",
      "      BatchNorm2d-11          [-1, 128, 16, 16]             256\n",
      "             ReLU-12          [-1, 128, 16, 16]               0\n",
      "  ConvTranspose2d-13            [-1, 1, 32, 32]           3,200\n",
      "             Tanh-14            [-1, 1, 32, 32]               0\n",
      "================================================================\n",
      "Total params: 18,865,024\n",
      "Trainable params: 18,865,024\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.70\n",
      "Params size (MB): 71.96\n",
      "Estimated Total Size (MB): 73.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nz= 100 #nz - length of latent vector\n",
    "filter_size=[1024,512,256,128,1]\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu # number of GPUs available. If this is 0, code will run in CPU mode. If this number is greater than 0 it will run on that number of GPUs\n",
    "        self.Layer1 = nn.Sequential(\n",
    "            # input is Z, going into a fully connected layer\n",
    "            torch.nn.Linear(in_features=nz,out_features=filter_size[0]*4*4))\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=filter_size[0]),\n",
    "            nn.ReLU(True), #out -1,1,1,16384\n",
    "            \n",
    "            # 1st T_convolution layer\n",
    "            nn.ConvTranspose2d(in_channels=filter_size[0], out_channels=filter_size[1], kernel_size=5, stride=1, padding=2, bias = False),\n",
    "            # The output of convTranspose2d is o = (i -1)*s - 2*p + f + output_padding => (4-1)*1 - 2*2 + 5 +0 = 4 (p=f-1/2)\n",
    "            nn.BatchNorm2d(filter_size[1]),\n",
    "            nn.ReLU(True), #out is -1,512,4,4\n",
    "            \n",
    "            \n",
    "            #2nd T_convolution layer\n",
    "            nn.ConvTranspose2d(in_channels=filter_size[1], out_channels=filter_size[2], kernel_size=5, stride=2, padding=2, bias = False,output_padding=1),\n",
    "            nn.BatchNorm2d(filter_size[2]),\n",
    "            nn.ReLU(True), #out is -1,256,8,8\n",
    "            \n",
    "            #3rd T_convolution layer\n",
    "            nn.ConvTranspose2d(in_channels=filter_size[2], out_channels=filter_size[3], kernel_size=5, stride=2, padding=2, bias = False,output_padding=1),\n",
    "            nn.BatchNorm2d(filter_size[3]),\n",
    "            nn.ReLU(True), #out is -1,128,16,16\n",
    "            \n",
    "            #4th T_connvolution layer\n",
    "            nn.ConvTranspose2d(in_channels=filter_size[3], out_channels=filter_size[4], kernel_size=5, stride=2, padding=2, bias = False,output_padding=1),\n",
    "            # out is -1,1,32,32\n",
    "            \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out_L1 = self.Layer1(input) #out_L1 is -1,1,1,16384\n",
    "        #resize to 4,4,1024\n",
    "        out_L1 = out_L1.view(out_L1.shape[0],1024,4,4)\n",
    "\n",
    "        return self.main(out_L1)\n",
    "    \n",
    "net = Generator(ngpu=0)\n",
    "\n",
    "summary(net, (1,1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAACRCAYAAAA8XyjoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztnXm8lVXVx38LuAQCJg4pIIqWJlYOOVspWo4paCWBY2pimobaa5I5ZVZUry+mmeYUDphiaU444JimiYiaAzmnoOIEKhIXMPf7xznPPt/9eM8dDwfuc9fv8+HDuuc841772edZv/1ba1sIQQ6Hw+EoBrot6wtwOBwOR+3gg7rD4XAUCD6oOxwOR4Hgg7rD4XAUCD6oOxwOR4Hgg7rD4XAUCIUZ1M1smJnNXg6uYz8zu31ZX0dR4H4tLty3SwdLdVA3s3+b2UIz+8DM5pjZRDPruzTPuawRQpgUQti5pe3M7DQzu6Ie14RzHmVm081skZlN7MBx3K9VUG+/mtknzOxiM3vZzOab2WNmtlsHjue+rYJl9MxeYWavm9n7ZvasmX23pX3q8aa+Zwihr6RNJG0q6cd1OGfhYWY92rHba5LOkHRJDS7B/boU0A6/9pA0S9L2kj4p6SRJk81sSAcuw327FNDOZ/aXkoaEEFaUNFzSGWa2WbN7hBCW2j9J/5b0Nfz9a0k34++vS3pU0vsqdczT8N0QSUHSQZJekfS2pJ/g+96SJkqaJ+lpScdLmo3vh0q6R9K7kp6SNBzfTZT0e0m3SPpA0t8lrSHprPLx/iVp02buK0j6gaQXy9f1G0ndyt99R9L92PZzkqZKmivpDUknStpV0mJJS8rnf7xKe50m6Ypcexxabo+/lT/fWtID5ft8XNKwVvjlDEkT3a/F8iuO/09J33TfFsu3kj4r6XVJI5vdrr0Pdls7iKQ1JT0h6bf4fpikL6gUMWxUbsC9cg1yYbkzbCxpkaSh5e/HS7pP0sqSBkt6MusgkhokPV92Rk9JO0qaL+mz6CBvS9pMUi9Jd0l6SdKBkrqrNOjd3UIHubt87rUkPSvpu/kOIqlf2Qk/LJ+nn6St8s5v5oFqqoNcJqlPuU0GSXpH0u7lNtyp/PdqLfilZoO6+3X58Wv5eKtLapS0gfu2GL5V6cfsP+VjzZDUt1kftvfBbkMH+aDsnCDpTkkrNbP9WZIm5BpkTXw/TdKosv2ipF3x3Rh0kK9ImqPyL3H5sz+p/FZR7iAX4rujJc3E31+Q9G4LHYTnPlLSnU10kNGSHq1yjPZ2kHXx/QmSLs8d4zZJB7Xgl1oM6u7X5c+vDZLukPQH923hfNtd0pdVotcamtu2Hpz6XiGEfir9wm8gadXsCzPbyszuNrO3zOw9Sd/j92XMgf0fSdmkzUCVwr8ML8MeKGlWCOGj3PeD8PcbsBc28XdLk0P5cw9sYpvBkl5o4ThtBc+7tqR9zOzd7J9Kjh9Q43M2BfdrbdEhv5pZN0mXq0QRHNXBa3Hf1hYdfmZDCP8NIdyvUvR0RHPb1k3SGEK4V6Vf2//Fx1dKukHS4BDCJyWdL8laecjXVXJAhrVgvyZpcLmj8/tX23jZzSF/7tea2GaWpHWr7B+a+GyBpBXw9xot7DdLpV/9lfCvTwhhfDPXXVO4Xz+GuvvVzEzSxSpRL98MISypcm1tgvv2Y1gentkekj7d3Ab11qmfJWknM9u4/Hc/SXNDCI1mtqWkfdtwrMmSfmxm/c1sTZXCsQwPqfSG8CMzazCzYZL2lHRVh++gguPL5x4saaykq5vY5iZJA8zsmLL0rJ+ZbVX+7g1JQ3Kd+DFJo8rXvLmkb7VwDVdI2tPMdjGz7mbWq6z9XbOpjc2sh5n1UimUy7Zvz4x8Hu7XZehXSeepNMm4ZwhhYWtvtJVw3y4j35rZp8xslJn1LW+7i0r00J3NnaCug3oI4S2VJg1OKX90pKTTzWx++bPJbTjcT1UKoV6SdLtKoWd2nsUqdYjdVJpc+b2kA0MI/+roPQDXS3pEJaferNKbUoIQwnyVJkL2VCkkfU7SDuWvryn//46ZzSjbJ6v0KzxPpfu7srkLCCHMkjRCpcmlt1R6Czhe1f16kkph6jhJ+5ftk5q/zZbhfl12fjWztSUdrpL8cE5ZX/6Bme3XyvttFu7bZfrMBpWoltnl4/+vpGNCCDc0dw4rk/CONsDMgqT1QgjPL+trcdQO7tfioiv5tjBlAhwOh8Phg7rD4XAUCk6/OBwOR4HQoTd1M9vVzJ4xs+fNbFytLsqxbOF+LS7ct8VHu9/Uzay7Sqm2O6k0O/uwpNEhhKer7fOJT3wi9OnTR5LU0NAQP//oo0q+AT9vbGyMdrZfBl43t+vbt5J/0K1b5Tfrgw8+iHbPnj2j/Z///KfJc/CaeC7uW76naM+bN6/J61iwYEGT53jvvfeiXZIaf9yu1k6S1Lt372gvWrQo2j16VFSKH3744cf2f++997Rw4cImtcXt8WuvXr1Cdr8rr7xy/Jx+4XXw8379+iXH4v3yPug/Hovtz3bL3VO0lyypSLjpS16TJHXv3j3a//3vf5u8Jh63V69eTZ6Dvieq+U6SVlpppWi/88470V5hhYocmufO2mPu3LlasGBBVc14W33bu3fvkPmHfqHPeK/N+ZXgM0cfLFxYUWPymeN9L168ONp8lvg5n5P8M0P/0Tf0N89H39OmzzjO0Gab5c/BY3G7aud7+eWX3w4hrKZWoCMa5S0lPR9CeFGSzOwqlaQ6VR/+Pn36aJdddpEkDRhQSaB6//33oz1oUCWB7Jlnnon2FltskRyLTnzuueei/aUvfSnafNAefPDBaA8eXMlBmD59epP78prYcddai/kS0mc+85loT55cUXd95StfifY//vGPaG+99dbRvvXWW6PNzkbnDxxYSXrjwCZJm2yySbTZBquvvnq033zzzY/tP2nSJDWDNvu1b9++Gj58uCRp5MiR8fPnn68IDd5+++1o06877LCDiDXWqORucHB76KGHov3WW29Fe911K3ki+YcoA9v2tdcq+SZDhgyJ9rPPPpvswwGDD3///v2bPO7nPve5aL/++uvRZr/jA8/t2U6StPfee0f78suj6i/xNwfDrG0nTJigFtAm3/br10/f/va3JaV9arvttov2G29UkjrZhuz/efzzn/+M9pprVuTZTz75ZLT5zG22WaUo4axZleTML37xi9GmX+fPnx9t/oDk7+OBBx6INv268cYbR5vjAPsBfca+wpcMPruS9MlPfjLafKH71Kc+FW0+4++++260DzvsMGbfNouO0C+DlKa/zlaa0itJMrMxVqrhPT3/RuJYLtFmv+bfch3LLVr0Lf3KN2dH50EtsgmbRQjhAkkXSNJKK60Usl/OYcOGxW1eeKFSaoG/1nvuuWe0//WvNAfhlVdeifauu+4a7VdfrWQVM8RdZ511ov3Zz3422nzT5hsg7Sy6kKQbb7wxuQ6+qfBYxG67VdYs4C8/3w622WabaFf7tc5HCXyT3WCDDZo8N8P37E2BkUB7Qb8OHDgwrL322pLStw6+qdx5ZyUJ7sgjj4z2Sy+9lD9utOlzRhyktnbeubK2wc033xxthtBsT765PfXUU9HedNNNk+vgm9/mm28ebb5tv/jii9FmqMyohH2TESL35ZuhlPr805+uZIR/4QtfiPbMmTOjnbUHr629oF+HDBkSNtpoI0nS0KFD4zbsP4ysGYVed911yXE///nPR5tv53z211tvvSbPwTd10nvsX3yWVlxxxWgzGs5/N3r06Cavg5Fk1q+llMr55je/GW2OFXwDZ9tIad/mGz3P/e9//zvaWdu3FR3pBa8qraWwpmpbp8GxbOB+LS7ct10AHRnUH5a0npmtY2Y9JY1SqdCPo3PD/VpcuG+7ANpNv4QQPjSzo1SqBdxd0iUhhKea22fx4sVxouOJJ56In3MicZVVVon2HXfcEW2GrlIaFlF1wuNyEu53v/tdtI877rho33PPPdHeb79KuYwZM2ZEm9QBw/L89XJSa999K3WOHn300WgzJPva174WbYZ8nBC7//77o832kFJa6KKLLoo2748T0hlH2hz90h6/duvWLYaTpBUYljLsZXs8/vjjybEYXp9wwgnRZh9hW911113RnjJlSrS33377aDNM/+pXvxpt9pX8hBrVJewjnCTkZNmqq1aqz/Je2XdIIfE4eb/y2l9+uTI/tu2220abIoCszapNFGdoq2/ff//92L7VRAv8nDTX7rvvnhyL/iMdyeeavqQ/OLlJaoqUF49zzjnnVLulhP5k/+TENef+OEFPGo4+I3VKyvfSSy9Nzn3iiSdGu1p7cgK8vcrEDnHqIYQpkqa0uKGjU8H9Wly4b4sPLxPgcDgcBcJSV78Qffv2jTPk1LFy9p90AZURDLukdCaaOtFf/vKX0SYdct5550V72rRp0T7++OOjzXCamlLqXo84Il105O677472hRdeGG2G3aR1GKJec8010WZ7kOIhtfSDH/wgOfff//73aB91VGWxG147VRJZGFstSae9aGhoiH7jrP5jjz0W7QMPPDDaVDPk1QnUjj/9dEU+zVCU4So1z6S8vvGNb0T72muvjTa11KSKeN1Sqq76+te/Hu05cyqL+uyzzz7RJv1y1VWVEuBUKDFMJ8WWT5BhHyHFRuqA1EPmz1r7dcUVV4x0FVU4pAhIPzJnYO7cucmxqM4iDUVNPxVLzBmhooSqOX5+8cWVKrp8FvJ+rXY++pXKFj5jvD9SQhy/rryyUnl3q622EkFKifkYHNvYz9urUvM3dYfD4SgQfFB3OByOAqGu9EufPn205ZZbSkqpEYYvDIkZupJGkKQrrrgi2lQ0MI2YqofLLrss2pwpZ8o8k5KY7n777bdHm2nfkjR79uxoP/zww9Hm7DZn/hmyM/wj1cSwi2oQho5SOmN/2223RZtUDimMLPOTJRZqgQ8//DAmOZG2YFo1E4mYXj916tTkWGxP0iy33HJLtA877LBoM+GG98WUefYJKhhIZ9DfUtpfSKcwwYy02p/+9Kdok64jLcNkJ95PvvwD/cq+R1rh6qsrK7FliT1LI7M3owOYxMZnlAoufs5+IKXPHJUjhx9+eLRJZZImY9LVDTdUFJikank+Jgjma0YxwYmqpoMPPjjap59+erRJA40dOzbapEwOOeSQaLPsCK8vD97r97///Whz3KBCqi3wN3WHw+EoEHxQdzgcjgKhrotkDBgwIBx66KGS0hohnClnyE76hCoHKa3Qxpl1Kj9Ih7C2B2tzMPQlxUP1BEO7PHXBWXDeB2ma7J4l6d577402Q3kqRb785S9H+/rrr482Q0EprTvDhBfWj2AtnIzOOv/88/Xqq6/WTCqx7rrrhp///OeSUmqFs/8M36l+YM0OSbrvvvuizXb429/+Fm3SVuuvv360WYCKtUpIC5AOYQJPngaiX6laIY3EBDgmqfC+SSfx+piAla8ZxLK1pGL++te/Rrspiu7UU0/VSy+9VDO/Dh48OGSUA5/XDTfcMNrsz6Qh2B5SSm3S/0w+o6Jn//33jzYT677zne9E+4wzzog2lU+sDZRXFpGi4pjAZEP6iX2Ex11ttUoVXFKcTFzKl+nmWMN6L1TxkB7ksUaMGPFICKHyZTPwN3WHw+EoEHxQdzgcjgLBB3WHw+EoEOoqaVywYEHkmkeMGBE/J5dGDovSMHLGUlroiDwpVwAin8aMM3KY5MPI4TNzlHXdzz777OQ6fv/730ebMkvWPmcmKOVLlF6xuBSlb6zHTS5SSrPluD+zFcnXZdxrLeqpE/Pnz4/FmFhTm77gnAGlqpz3kNL5DhYzI89ZjWsn//w///M/0Wb2HiWilKWRH5fS+vvsR/QN+XwWhyJfzsxinu9nP/tZtCmNlKTTTjst2ixSRokcMzkz7rXWfl28eHGcE2ChKt4T/c3n6nvf+15yLPbDn/70p01ux+eP8kFmylIyyD5BGSIlvWeeeWZyHcwuPuuss6LNuR2OLbwnzs2wH/HZ22mnnaLNfi6lPuOzwfuotoJZW+Bv6g6Hw1Eg+KDucDgcBUJdJY1Dhw4Nf/zjHyWlhZyyxW2ltAYxKRCG5VIq52MdaRblYiYnwyUWSSJ1wzCbBX4oRWIIJaVZiZQ5UTrF7DHSKZRKUgrIpb8Y1pMqyp+bS4RRdscQMJNYTZgwQbNmzaqZ9G3gwIFhzJgxktJ68QybWcSLRZLy/Y9yQGbXUepKSSPbmXQUqRH6mMf585//HO18/W8ufMx2zjKipZQ6YgjOomT0C+WlDK3z0rdHHnkk2lzSjEvEUbKZ9clx48bphRdeqJlf11lnnXDqqadKSuV8pCRIf1FWSMmelPom348zkKLba6+9ok3qhsdhkTE+Y8y8pvRZSmlRSpApuSTlRSqG0lZSJuwrpKl4zPy1c6yhbJn0M+XZI0eOdEmjw+FwdEX4oO5wOBwFQl3pl1VXXTXssccektLMQYaSVCqwrvjee++dHIvhD+uYsxYy92fmG6kOZsexMBiz2Jjhx8w6KS0uxSXzGIYxfKQShm3Agl7MJGP4yCJTUpr1SlqByg22QaYymjRpkt54442ahenMFCbVcfLJJ3ObaGchvfTxgksM86mM4L2yvjnrV/NeSdGwWBMpoVNOOSXa+SXQqGSi4mXnnXdu8tpZWIwhNIuJkZpiAbA8mOnIDGa2B/tn1p8vu+wyzZkzp6a0GrOhM5DWZLEtqtiY5S2l/YJty75OapLKN1Igw4cPjzZ9SbqHGdUcT6TUB8xSp7qOfZXbk1bj+gyko6oV5pPSNmD/pFqNFB3VNieddJLTLw6Hw9EV4YO6w+FwFAh1TT7q2bNnTPJgyEFKgzPJpBHyYOjFpdKoVGAyAwsB8RwMr1hrmXWiq4WYUqqGYXg8efLkaDNsY3IBqRXOlLP2NBOOjj766OTcLP7DuvBsW6osMnoir7boKLp16xaPSerggQceiDb9QnqCK8hLpdrsGX71q19Fm4WfqA7hcoQM5Ul7UB3F0JzJJ6TLpLTd2O5UOFElQZqEKoftt98+2lzOjMl3VHpIaYINfcV2Y7/LlF35pds6Ci5TSKUJVS6se051CBUdUlo//sgjj4w2nwc+rzxWVixOSlVsVLLwWaBfqB6SUj/xuabShO3PInpU3JECJGXIsSyvvCFlXK32PWk8tnlb4G/qDofDUSC0OKib2SVm9qaZPYnPVjazqWb2XPn//s0dw7H8wf1aXLhvuzZaQ79MlPQ7SZRejJN0ZwhhvJmNK/99QksHWrhwYVQSUPHCMIWhNZMUSElIab1mzj4zaYiKkG9961tNHqsabcGw66abbop2fukxKluokqEygiE763+wfjtpC1INvFaqAKR0Zp+1K6gEYDiYUQxl6mqiauTXJUuWxCQpJo+xDak4Ih1CWkxKE1BIxXBpLyoESEMw7GZITNAvpGIYckupqolLqLGvjho1KtpUdJAaZJ0U1gUh1fCHP/whOTdr21AlwfCfoXnWtmjjiaqBb80sUjpM7iF1xKQr0kukNaU0wZBUHNUsTJRj/X0ud0kVW7Wa5lTskPaR0n7EJSBJkc6bNy/apPH4XFG9ROqU6rs8BcWkR6oOq60h0V5lYotv6iGEv0mam/t4hKTs6b1U0l5ydCq4X4sL923XRns59dVDCFku7hxJVcuJmdkYM5tuZtM5keVYLtEuv3ICyLHcolW+pV+pFXd0HnRY/RJCCGZWNU4IIVwg6QKpVPslK4nKMJshDhUvDK05Oy2l9U1YcpX0BkNUziqTMjn22GOjzTK6nCXniudMvJDSEHCfffZp8nys5cJ6E6RZGNqRcmEYynom+X2YFMOl9Ni2mVqA11YNbfHroEGDQtYOrPFC5Q6Th1ibI69+Ye0eJl+QYqMKhKoVlrCluuDqq6+ONkN/htx5BQqvixQBwT5Cn+XrA2Wg0oNKq3yNEFIutFmKmKF5vmxwS2jOt/TrGmusETJ1C6kjUn1M7OLn7LdS+rwzKYdL9JEiZd9hMtABBxwQbdInPCZ9QfWRlNJyxx13XLTztWoysP4Qj8V+TpqQ/S5LtMzAl1qq6Fg/h88xx6m2oL1v6m+Y2QBJKv//ZgvbOzoH3K/Fhfu2i6C9g/oNkg4q2wdJur6ZbR2dB+7X4sJ920XQIv1iZn+SNEzSqmY2W9KpksZLmmxmh0p6WdLI1pxs0aJFsTYClQcMS6mY4Gx6ftV5hi9MImEIzhCQVAc/54w768swkYjlU/O1JFgXhIocho9UzLCULssJk1LgTD6TYNhmkjRyZKXZx40bF+0TTqiIGrgKT0ZZNTY21tSvH330UQwtSVtQpVJttp+lbKWUBmFdF5ZhZjIKKS/e9zXXXBNt+pu+ZL/L03u8RpYKZn0g+p400G677RZt+ox0DfsBV+rJ78NrpzqLZXizuiXZM1Er366wwgrxnKTCuMrQb37zm2izlPSTT0Y1paSUciPFSh/zWSSdQl9wJSPSkexfbL98QhZ98KMf/SjaXMGJaiSWgib1SpqFzyjVVfly4aRvqJY68cQTo52VJpekLbbYQu1Bi4N6CGF0la++WuVzRyeA+7W4cN92bXhGqcPhcBQIdS29u/7664ds4WbONjPZgqJ+JluwzkN+H9af4P2w7gNraFBFcNRRR0WbpW1ZOpShIxcFlqSDDjoo2kwu4DXxWKQk+DnLgFJKxvK8o0enL2Ccdef+DDmpdMlooLPPPluzZ8+uWYnWtdZaK2ShLOtpMOGIFBSpDpYqlVLFElcWomqISphqVAfr+5AWIN3DZDEuBC2lFAETkdh31l133WiTdqLaiSvcUJmVLegsfbxGSL6+UAZSiFTMZL7/9a9/rVdeeaVmfl1//fXDueeeKyntR1RxsA+TbqDaRUqTDUk1kgJjHyEVymQgJnaREiKtRgVVPmmR4wCPxb5GKo3lcplQVW2lK7ZBXgVFn5NO5vVSFcWxYv/99/fSuw6Hw9EV4YO6w+FwFAh1Lb37zjvvxFlflsok5cLwgzPoDGmlUqiZgYoZbsfZaiadMAOSCUdUrPz2t7+NdhaCSmkihJTWbGG5XVIErP/BpAXWPWG4z6QKzsRfe+21ybkZ0jJMZPhOOwtFa116d9GiRTGMJu1EBQNDV9JGTK6RUuUP25YKA4bjVKBMnDgx2j/84Q+jTSVMNVqG6gcpbTeWAGZCCGvxUHnDsJm1cEghke4ZP358cu5f/OIXTR6L1BRpi8yfpBNqgcWLF0dKa9iwYfFzllSeNm1atP/yl79Ee8KECcmxuDoZVSvs9zvuuGO0WZOpmqKE9WSYJMQVyFgLSkqfZZbxpaqJCVE77LBDtLlqGZVIpBBJB5J6k1K6jglH2267bbTZnqSp2gJ/U3c4HI4CwQd1h8PhKBDqSr/069cvhjMMFTlDzfCWC9HmQ0smoDCUYS0WJt4wiYDlfUndMGxjcgATQJgokL8uHosqCdIspAtYUpSJBgwrSRXlV0JhCVsqEjibTmTUVj4s7CjoV1IMDK2pTuA95csJk96ieoY+oy9JT7HsKZUm1VbG4vFJX0nSHXfcEW3WN6EChVQa1R4M68eMGRNt0k5cJJuLSEtpOM8kF1JTvL9M5cX7qQV69eoV1S1UexBUmFEhQ2WRlNZ1YRIO+yKpByqCeF9U/bDP83z0dz5pkeo4lqVmoiOpmKlTp0abfuXzyueQVBGT1qRUJUMFGGlA+pvJWG2Bv6k7HA5HgeCDusPhcBQIdaVfQghRfcCwhmEGw2MuUHvjjTcmxyLlwmQBqiwYUrGmCM/HmXiGUVmJYClVi4wdOza5DiYtMBmCYRhpHZ6PdUt4XIaYpC1I6UhpzRCG86R1mOTF2i+1RGNjYzw2Q+ILLrgg2iwpygSge+65JzkWQ3MqR7gPa4+QrmNCGlUOpLO4YDDVMizlLKVJVBtuuGG0udA1KRuG2lR+UIVD5Q2pOi5yLqWldKmAYPliUjSZ73n8WmDx4sWRGqByg4ocJuZRjUWaUUqfdz67TNDh6lNUvvG+SVXw+Z4xY0a0ScVwDJFShRr7BdU9VF2RUuLzxoQ01ptiiWKq3qS0DzNBkOMOaRnWqWkL/E3d4XA4CgQf1B0Oh6NA8EHd4XA4CoS6curz58+Pq5Lvu+++8XPymeRYKSHKZ4YdffTR0SaHRo6VvNUxxxwTbS5/R6kW5U/kUcnv5QugUab23e9+t8lrIu/OjFdm5nFfSv4oXdt9992Tc5OX5X2Ti2P2Ja+1lmhsbIwZfVwNfdSoUdFm1hzrY+eXciPIQTLzlLXuyemy1j0zGCk7JX9JOV5essfsRs4N8HqZ3Uiemf2Fclby9JzXyC9bxn7I+SL226bWD2UGby3Qo0ePOG/AYlucs+G8BGWL5NCl9Pmjz5iBzPkK9mFy5+xHbHP2D0oH83XdOQ/C4nxc9pHHonSRbXDqqadGm9njrJPP7HipOqfOQnR8XjmP0Rb4m7rD4XAUCD6oOxwOR4FQV/qlf//+MUSjrClbjkuSttlmm2gzvOXK31Iaptx0003RZrjEMIqyQtZqpkyJ2Y0MnShVJHUgpRK5yy+/PNoMtRlCU5rJcPW8885rcl+ufk7JmJRSVaQY2J7M0MxC9qURpmeZvJSAUV7KolqkG1jPXkrDY4bdlKEya5X17LkUGDOCt9tuu2iT2qqWXSql9Bkzipk5yqxH9lvKDbkmAAtFUQbHDEYpzRy+/fbbo82+RzoyK0ZW60JtPXv2jNnezLBle5KmpByPUlwprbtOvx5yyCHRptyQ0kC2AX1frXAdM8l53VIqk+XykxxP+PxQZslnif2D/YiF4UgDSemaByzIRv/nKZv2wN/UHQ6Ho0DwQd3hcDgKhLrSL4sXL44z1gxfhg8fHm3OMDOUefDBB6sel2E+i1mxYM+Pf/zjaDNsZtEphnCXXHJJtEmBMGNSkq6++upoMzOQoTLDTdZnJm1EKobZicyEzYMz+1RlkC5iG2SoNf2y4oorxnCZmbSkLRhiUgVA9YOU1sCnUoXUE33DLEr6lYoEKg2YYctMRSpcpNQHzOrkOgAMlRm+UwFBFQ3VK1SA5PsUi5eR6iD1wHreGRVTa3XTokWLYntRkXPLLbdEm/dHmnHmzJl7kz6eAAARrElEQVTJsVhrne3Ae+I+hx9+eLR5bj4nbCcWWqNC7ZRTTkmug9QMC6Sxr1Wrb87+RbUM6Rf6NZ8Ryr7KsYLUKSlgFidsC1p8UzezwWZ2t5k9bWZPmdnY8ucrm9lUM3uu/H//lo7lWH7gfi0m3K+O1tAvH0r6YQhhQ0lbS/q+mW0oaZykO0MI60m6s/y3o/PA/VpMuF+7OFqkX0IIr0t6vWzPN7OZkgZJGiFpWHmzSyXdI+mEJg4RsWTJkki/UAHBkJvqBCpF8vWZ+TeVEVQx3HvvvdGmKobUCJOgGBZy5prhXL5WOZdZO+uss6LNQk4MV3nfnA0nfcLZ9zPPPDPa+aXfSAVcfPHF0WaS0nXXXRftLJlhyZIlNfXrBx98EIuNsRgVE3pIgTDMZtgspWoiJl6xX3D5Q6pnWH+fiVks6sSCUCeffHKT20ipioeJTKSRrrzyymhfddVV0aZSi4oX9iPWEc8n1lHFwms86aSTos1lGLMicY2NjTX1a7du3eJzRvUS74PUIqmtPO13wAEHRJvF56oVOfvJT34SbSY7nXHGGdEmvUEqjHQp+7+UPjOkdFmgi/2LyXT33Xdfk+dmP+UxqeCRUhqPFB3boFriYFvQpolSMxsiaVNJD0lavdyBJGmOpNWr7DPGzKab2XR2AMfyA/drMdFRv+YzXR2dA60e1M2sr6S/SDomhPA+vwuln+7Q1H4hhAtCCJuHEDbPv2k6lj3cr8VELfxK3bmj88DytUya3MisQdJNkm4LIfxf+bNnJA0LIbxuZgMk3RNCqF7IQ9J6660XslrTnPXmTDCVDby2fB0E0h6cPWYIyBlqJntMmTIl2qRuqMTg0mhUi+TVGlTrUNXBmhhUKvCtlrP6VIqwFg5nwEkvSGkCF1dTp2qISwVm4eNFF12k1157zWrl16FDh4Ys8Yc1TViLhaoAUljsB1IaRlOZxPapFjZXWxaM7URqim+iWU2iDExQY/+iv6mQYWIK+yqVLVQ2sN4Hk82kNOyutuQeKcQswenMM8/UrFmzaurXLFmHlAZpBFJFVJbQX1KqNGJbN6XikVJfkn5hP2d7UL1CapaJYPnrYv199hFStdy/2hJ0VKGxz+eTytgn2e9Zx4fjGs+93377PRJCqHBHzaA16heTdLGkmVkHKeMGSVk630GSrm/NCR3LB9yvxYT71dEanfqXJB0g6Qkzy3KkT5Q0XtJkMztU0suSRi6dS3QsJbhfiwn3axdHa9Qv90uyKl9/tcrnTWLJkiV68803JSn+L6UJAiNGjIg2Z6EZ8klp+EMKhRQF6zywBCqXlKMyZeTISj/nvgy78pNHXNmeah3uQxUIqaZJkyZFm+oOJihREcAEDilNkuDyYQzzSeVkCTkNDQ019etbb70Va9ewlg5ra7CMMiks3p+UqkVYWpVlfElbsXwxqQ7W4SEtxloepBTyNCQTw0gJUanAJC/eH2vQMHmI9T5YC4X1U6Q0HGftElIxtDPVVu/evWvqV5bK5vUy4Y5tQ+UGk7+k9Hngcn9c8o0UzfXXVwIJ0iw8DhVVpDWbS+DhkpBsZ6pizj///GhTpcQkSVIuXKqRYxH7v5QmLzHxiX2E41p756q8TIDD4XAUCD6oOxwOR4FQ19ov3bt3j+ETQ2Um23AGnLPQrPkgpeELwzCG7Ax3SLkwxOFKRLwOlv2l+oXJCFIawrF2DGu2sHQoExU4U041A9UTTFBiXRUpVVBQ1cE25Aw6r6+W6NevX6SVOMNPaooUFJO86DtJOuKII6JNhQDbjf2CiSykN6iiqtYGVLKwdK6UhvncjjQE1Q2kuUjFsAQt66TcfPPN0c6vzkP6gOolUpakPbL+3N7V56uhoaEh9lHSU7xv0p2zZ8+ONttASpOXWFqYtBppOSahkW4jHcJz8LknFZOvh1Nt9She+y677NLkOe66665o877ZNuynzZXKZl9g21Dp097n1d/UHQ6Ho0DwQd3hcDgKhLrSL/PmzYsz51RJMAwiDcGQ9pxzzkmOReqCVAxrtjD0uu2226I9bNiwaHPmmjPjDNNYb4I1QaSUCmDCEcMwliRl8gMpISZVsI4L7ydfI4R0ESkiJrOQasrUHlRw1AKNjY3RB7xX1vThvZJeyK8kxTCYtARLJ59++unRZn0fKmlITZGiId1Dpc1aa62VXAcTjpgsRXUDw2uucES6gEkt9AXVIdxGSuk39i/Sb1wlKPN3rUvvLly4MKqqmFxFmmf8+PHRZt2fPP1CyoU1c0hnse+wzgr9RGUYnyVSJuw3VChJ0qabbhptJnnRB0yookKKPub1sU+wrzCRSKq+IhbHI1JQbI+2wN/UHQ6Ho0DwQd3hcDgKhLrSLyussEIMJ1makzUYqGA48MADo51f+YhhG8tYkkLhzDopFCoxSFswfCR9wjownH3P7ikDy68yKYb7MGGoWljPmXHWP8mHklSaUKHBMH233XaLdtZOeQqpowghxHskXcSZ/Fwdi2jnqSDWU2G7UTlAv1K1wkQT0ikzZsyINvsXKYX8gr+sH8Lj0h88FsN69lXSjKSWWGqZSS1SGnZT8cIVdqgsyvoO65/UAj169NAqq6wiKaWBqABjQhyTh2699dbkWKQKSWk0tTC6JI0dOzbaTPgj5UVKtlrZX1JhUtr3t99++2iTKqEvSQEzUbGp5C8ppRNJ7UrpOMXnle1J5U57q2T6m7rD4XAUCD6oOxwOR4FQV/rFzGL4w7oIDGU4u00qJZ9YwVCUIRzDboZhl112WbSZEMIQbOrUqdGmQmOzzTaLdn6RYIbUDJ1IC1CtwXslvcAQn2EaKZr8qktMyGICRLXZ+CzsrTX90rdv30gHkCah0oTtzFoeZ599dnIs0hKsicE2IWXGlaFY34cUG9UkrP3CBBeqj6Q0PKZfScuxHbkIMuk69mEqQrhYOhU8Uhq2U/FExRf7Ydae7Fu1QENDQ6QGeB1MCmPyHlUtfGaktK9TnUUalSolUohUqZBy4XGoJiHyCTzcjmMKr+/cc89t8j5I0VFpxX5AKiVfT4hUGqlejmW87/Y+p/6m7nA4HAWCD+oOh8NRIPig7nA4HAVCXTn1jz76KEp+yP+RT6bEjQW5yHFL1VeaJ//J+ubHHntstMkJkjN77LHHok2+jsfnsmxSypuRJ2VBMJ6b103JEiVV5AHZHpTy5Y/FeQK2FbnXjK9jtmStkPHclDGSO6f8ktJM1rGW0qxItgO5W2YQsxgS25MFuaot9cf5hnxNc3KvlPOxPVmnnXXPWVyKEjxKHblNvuY3v8skhVKaoUupXtbn83LbjqKhoSHOS5HrJTdMCSufq3zxOfLX7NMsbMa5Es6tsH1Gjx4dbXL43J7nzq+zSqkq5984/8PsVJ6DS+/xWrk8H8cNzm1Jqc+YtUrJLceB9mYI+5u6w+FwFAg+qDscDkeBUFf6ZcmSJVEaxQI/lDIxjOISbZQnSmmmF8Mz1h9n3ecLL7ww2jvttFO0KX1jRhtlRqQ2SNFIqcwpW9JNqp6tyOMy5CMVwG0oY8tnx1Gex8xWnpsrvGdUQK0LPzU2NsZCRJQhkpIgzcKCS2xzKZV6MXuTmabMSLzooouiTaqD7UZJHENzUgfXXXddch0suDVz5sxok0ZgQSlmTHIZRmaUsqAXsy/zYTr7MMP0jTbaKNqU6Gaf17qe+oIFCzRt2jRJqVSv2pJrlGxS6itJjz/+eLSZgcm+SOqUy9kxw5a+JH3C5S5JD7EYmJTWQSedxSxS9jX2lylTpjR5btKBlGgyA1VKKRs+u6QEqxUVbAv8Td3hcDgKBB/UHQ6Ho0CoK/3Su3fvGCoypOKsMhUrpFXydbcZlrKYD8MiZiFWK6TF8I91jakgoeIlX3ebs+tU61RTInB7qgBmzZoVbYbZVEIwhJXSsJ3UDJUmvKasDfOZbh1F7969ozqFhbsOPvjgaHN5OSqOeH9SWpSK2alUmjz88MPRZjhNqonHYZuT6iOVRtpHSvskFSgM50kVUlHEY1FtQV+QnuCSdVLqP9KO7COkFjOlTq392rdv3/jckPIiXUDlBykrKsGklOqqtlwfn2n2bT5/PA77PKkn0rZ77713ch1U6NAfpD1I3TGzmM8lqTdmJlPZ1VyNfqpcmDnKdQfaq2Zq8U3dzHqZ2TQze9zMnjKzn5Y/X8fMHjKz583sajPr2dKxHMsP3K/FhPvV0Rr6ZZGkHUMIG0vaRNKuZra1pF9JmhBC+IykeZIOXXqX6VgKcL8WE+7XLo4W6ZdQiumyGKSh/C9I2lHSvuXPL5V0mqTz8vsTVL9Q4cGwkqEylREMSaV0uTKGfaRpSK2QFmCBJ27PokwsYETlBsN3KZ39Z8IRw8GmEoCk6nXWGY4xgSe/6ny1ldHZVqSgMhqhW7duNfVrCCHeI6kwqiToV4bH+SJU3J/3Xm35MCaHcPtqS4GR8qLaIk+BUBVFH/M+qHJhyM7EJapf2D+4DdUTUhq28xpJ5WSqFKnSd3r16lVTvy5YsCAmxjCphglDTNRh2+SX6CPNxkQvJoyRamKiFmkuPldsN1IxVLLk1VWkX/i8Uz1DtRMpF7Y574E+qrZ0pZSOU1Tx8Bllv2CbtwWtmig1s+5m9pikNyVNlfSCpHdDCNmoOVvSoCr7jjGz6WY2nTfvWPaolV/5A+1Y9qiVX/PzWI7OgVYN6iGE/4YQNpG0pqQtJW3Qwi7c94IQwuYhhM35xuNY9qiVX6nRdix71Mqv+VR/R+dAm9QvIYR3zexuSdtIWsnMepR//deU9Grze5dqeWRhB2fH+UbAEJMJSqRopDTcYsjDMIpif9Zv4efclzRJtfrmVAFIaaIDZ7c50FVL9mHtFlIxVG5w33woyeXUuA9DO95rtdn0jvq1sbExJoPRLwy5GXry8/wPPe+XdAoHGPqYtAnpKPYXtgH9RaVBvu42/2bfoY/32GOPaJM2ovqFyg3eKxU1+WUKmURFn5P6Y9tk7ZlfGrCjfu3Ro0f0J1UgTLAh1UdqI58ox2iO/ZMJS6RW+Dl9wfbnWMHr4zY8l5S2Lekw0me0ScPy2SdtxM9JIeWfV9Ix1SjS5uib1qI16pfVzGylst1b0k6SZkq6W1K2SsNBkq5v+giO5RHu12LC/epozZv6AEmXmll3lX4EJocQbjKzpyVdZWZnSHpU0sVL8TodtYf7tZhwv3ZxWK0TFpo9mdlbkhZIerulbQuIVbX83PfaIYTVWt6sdSj79WUtX/dYLyxP9+x+rR2Wt3tutW/rOqhLkplNDyFs3vKWxUJXuO+ucI95dIV77gr3mEdnvmev/eJwOBwFgg/qDofDUSAsi0H9gmVwzuUBXeG+u8I95tEV7rkr3GMenfae686pOxwOh2PpwekXh8PhKBB8UHc4HI4Coa6DupntambPlGs6j6vnuesFMxtsZneb2dPletZjy5+vbGZTzey58v/9WzpWZ0FX8KvU9Xzrfu2cfq0bp17OcHtWpbTl2ZIeljQ6hPB0szt2MpjZAEkDQggzzKyfpEck7SXpO5LmhhDGlx+Q/iGEE5bhpdYEXcWvUtfyrfu18/q1nm/qW0p6PoTwYghhsaSrJI2o4/nrghDC6yGEGWV7vkp1NwapdK+Xlje7VKVOUwR0Cb9KXc637tdO6td6DuqDJLHUYtWazkWBmQ2RtKmkhyStHkLISvbNkbR6ld06G7qcX6Uu4Vv3ayf1q0+ULiWYWV9Jf5F0TAghWUWivDqNa0k7Kdy3xURR/FrPQf1VSVwzrFU1nTsjzKxBpc4xKYRwbfnjN8rcXcbhvVlt/06GLuNXqUv51v3aSf1az0H9YUnrWWlV856SRkm6oY7nrwusVKH/YkkzQwj/h69uUKmOtVSsetZdwq9Sl/Ot+7WT+rXepXd3l3SWpO6SLgkh/LxuJ68TzOzLku6T9ISkbKmhE1Xi6CZLWkulcqYjQwhzmzxIJ0NX8KvU9Xzrfu2cfvUyAQ6Hw1Eg+ESpw+FwFAg+qDscDkeB4IO6w+FwFAg+qDscDkeB4IO6w+FwFAg+qDscDkeB4IO6w+FwFAj/DyK+jjhlN8hEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116a3b7f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "Z = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "\n",
    "Z = torch.from_numpy(Z)\n",
    "Z= Z.float()\n",
    "gen = Generator(0)\n",
    "criterion = torch.nn.BCELoss() #binary cross entrpy\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "generated_images = gen(Z)\n",
    "\n",
    "generated_images = generated_images.detach().numpy()\n",
    "\n",
    "print(generated_images.shape)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(generated_images[0].reshape(32, 32), cmap=plt.cm.Greys)\n",
    "plt.title('Random picture 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(generated_images[1].reshape(32, 32), cmap=plt.cm.Greys)\n",
    "plt.title('Random picture 2')\n",
    "    \n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(generated_images[2].reshape(32, 32), cmap=plt.cm.Greys)\n",
    "plt.title('Random picture 3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, now we have a random picture generator, taking a [batch_size,100] array as input. There is a nice article about upsampling a picture, nice to read: https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Discriminator ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, this is the opposite of our generator. From a 32x32 picture, we will use the convnet to output a number between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./discriminator.png\" width=\"700\" height=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"./discriminator.png\",width=700, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the ouput size of a convolutional layer is $\\frac{n+2p-f}{s}+1 $ (n= input shape, p = padding, s = stride, f= filter size). \n",
    "Padding shape is:\n",
    "* \"Valid\" : n-f+1\n",
    "* \"Same\" : $p=\\frac{f-1}{2}$ ; $\\frac{n + 2p -f}{2} +1$\n",
    "\n",
    "There is no common rules regarding, the filter size, just a 2^n size. If I keep same kernel size as the generator, I will end up with 2x2x1024 as output of the last conv layer. This is maybe not good, but let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 16, 16]           3,328\n",
      "         LeakyReLU-2          [-1, 128, 16, 16]               0\n",
      "            Conv2d-3            [-1, 256, 8, 8]         819,456\n",
      "       BatchNorm2d-4            [-1, 256, 8, 8]             512\n",
      "         LeakyReLU-5            [-1, 256, 8, 8]               0\n",
      "            Conv2d-6            [-1, 512, 4, 4]       3,277,312\n",
      "       BatchNorm2d-7            [-1, 512, 4, 4]           1,024\n",
      "         LeakyReLU-8            [-1, 512, 4, 4]               0\n",
      "            Conv2d-9           [-1, 1024, 2, 2]      13,108,224\n",
      "      BatchNorm2d-10           [-1, 1024, 2, 2]           2,048\n",
      "        LeakyReLU-11           [-1, 1024, 2, 2]               0\n",
      "           Conv2d-12              [-1, 1, 1, 1]          25,601\n",
      "          Sigmoid-13              [-1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 17,237,505\n",
      "Trainable params: 17,237,505\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.16\n",
      "Params size (MB): 65.76\n",
      "Estimated Total Size (MB): 66.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "filter_size_discri=[128,256,512,1024]\n",
    "picture_nb_chanels = 1\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu # number of GPUs available. If this is 0, code will run in CPU mode. If this number is greater than 0 it will run on that number of GPUs\n",
    "        self.main = nn.Sequential(\n",
    "        \n",
    "        # 1st convolution layer\n",
    "        nn.Conv2d(in_channels=picture_nb_chanels, out_channels=filter_size_discri[0],kernel_size=5,stride =2,padding = 2),  \n",
    "        #out: (n+2p-f)/2 +1 => (32 +2*2 -5) /2 +1 = 31/2 +1 = 16.5  =16 (take the round)\n",
    "        #nn.BatchNorm2d(filter_size_discri[0]),\n",
    "        nn.LeakyReLU(), #out is -1,128,16,16    \n",
    "    \n",
    "        # 2nd convolution layer\n",
    "        nn.Conv2d(in_channels=filter_size_discri[0], out_channels=filter_size_discri[1],kernel_size=5,stride =2,padding = 2),  \n",
    "        nn.BatchNorm2d(filter_size_discri[1]),\n",
    "        nn.LeakyReLU(), #out is -1,256,8,8\n",
    "            \n",
    "            \n",
    "        # 3rd convolution layer\n",
    "        nn.Conv2d(in_channels=filter_size_discri[1], out_channels=filter_size_discri[2],kernel_size=5,stride =2,padding = 2),  \n",
    "        nn.BatchNorm2d(filter_size_discri[2]),\n",
    "        nn.LeakyReLU(), #out is -1,512,4,4\n",
    "            \n",
    "            \n",
    "        # 4th convolution layer\n",
    "        nn.Conv2d(in_channels=filter_size_discri[2], out_channels=filter_size_discri[3],kernel_size=5,stride =2,padding = 2),  \n",
    "        nn.BatchNorm2d(filter_size_discri[3]),\n",
    "        nn.LeakyReLU(), #out is -1,1024,2,2\n",
    "            \n",
    "            \n",
    "        # 5th convolution layer\n",
    "        nn.Conv2d(in_channels=filter_size_discri[3], out_channels=1,kernel_size=5,stride =2,padding = 2), \n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "        \n",
    "        \n",
    "dis = Discriminator(ngpu=0)\n",
    "\n",
    "summary(dis, (1,32,32),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a random 32x32 picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4863418936729431\n"
     ]
    }
   ],
   "source": [
    "in1 = torch.randn(1,1,32,32, dtype=torch.float)\n",
    "\n",
    "dis = Discriminator(0)\n",
    "\n",
    "criterion = torch.nn.BCELoss() #binary cross entrpy\n",
    "optimizer = optim.Adam(dis.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "return_label = dis(in1)\n",
    "print(return_label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay seems, we correctly generate a binary prediction based on a 32x32 input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we enter the real GAN mechanism. As a reminder we currently have a generator and a disriminator. How to handle the complete GAN process? Here are the main steps:\n",
    "1. Generate a batch of fake images thanks to our Generator\n",
    "2. Train the discriminator mixing the batch of fake image + a same number of real image. We also add the label 0=fake, 1=real. Optimize the weight of Discriminator network.\n",
    "3. Generate picture with the generator, infere them in the discriminator network and improve the genrator network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "num_epochs = 50\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "num_batches = int(len(trainloader) / float(batch_size)) # around 1093 batches\n",
    "\n",
    "gen.apply(weights_init)\n",
    "dis.apply(weights_init)\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=\"cpu\")\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(dis.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(gen.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(generator, epoch, batch):\n",
    "    r, c = 2, 2\n",
    "    gen_imgs = gen(Variable(torch.randn(batch_size, 100)))\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            #print(cnt)\n",
    "            #print(gen_imgs.detach().numpy().shape)\n",
    "            #axs[i, j].imshow(gen_imgs[cnt, :, :, 0].detach().numpy(), cmap='gray')\n",
    "            axs[i, j].imshow(gen_imgs[cnt,0, :, :].detach().numpy(), cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/mnist_%d_%d.png\" % (epoch, batch))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0/1875 [D loss: 1.508353] [G loss: 8.994370]\n",
      "Epoch 0 Batch 1/1875 [D loss: 0.087169] [G loss: 8.075891]\n",
      "Epoch 0 Batch 2/1875 [D loss: 0.266323] [G loss: 14.561055]\n",
      "Epoch 0 Batch 3/1875 [D loss: 3.198386] [G loss: 0.844514]\n",
      "Epoch 0 Batch 4/1875 [D loss: 6.916126] [G loss: 8.536271]\n",
      "Epoch 0 Batch 5/1875 [D loss: 0.026196] [G loss: 9.340776]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x11b3583c8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/usr/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 12307) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f53a99065dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Calculate error and backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0merror_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0merror_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizerD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for batch_number, data in enumerate(trainloader, 0):\n",
    "        #data = list of 2 elements : 1st element = batch_size x pictures(tensor) ,label : tensor([4, 5, 2, 8])\n",
    "        \n",
    "        #1 Train discriminator\n",
    "        #1.1 on real labels:\n",
    "        real_data = Variable(data[0])\n",
    "        if torch.cuda.is_available(): real_data = real_data.cuda()\n",
    "        optimizerD.zero_grad() #Clears the gradients of all optimized tensor\n",
    "        \n",
    "        prediction_real = dis(real_data) #train\n",
    "\n",
    "        # Calculate error and backpropagate\n",
    "        error_real = criterion(prediction_real, Variable(torch.ones(len(prediction_real), 1)) )\n",
    "        error_real.backward()\n",
    "        \n",
    "        #1.2 on fake data:\n",
    "        fake_data = gen(Variable(torch.randn(batch_size, 100))) #generate innput vector\n",
    "        prediction_fake = dis(fake_data)\n",
    "        # Calculate error and backpropagate\n",
    "        error_fake = criterion(prediction_fake, Variable(torch.zeros(len(prediction_fake), 1)) )\n",
    "        error_fake.backward()\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "        \n",
    "        #2 Train generator\n",
    "        fake_data = gen(Variable(torch.randn(batch_size, 100))) #generate innput vector\n",
    "        optimizerG.zero_grad()\n",
    "        # Sample noise and generate fake data\n",
    "        prediction = dis(fake_data)\n",
    "        # Calculate error compare to supposed real labels\n",
    "        error = criterion(prediction, Variable(torch.ones(len(prediction_real), 1)))\n",
    "        error.backward()\n",
    "        # Update weights with gradients\n",
    "        optimizerG.step()\n",
    "        print(\"Epoch %d Batch %d/%d [D loss: %f] [G loss: %f]\" % (epoch,batch_number, len(trainloader), error_real+error_fake, error))\n",
    "        if batch_number % 50 == 0:\n",
    "            save_imgs(gen, epoch, batch_number)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU is available, insert .cuda() when creating generator and discriminator objects, also when creating tensors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
